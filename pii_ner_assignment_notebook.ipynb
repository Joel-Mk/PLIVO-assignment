{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6a52d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: data/train.jsonl, data/dev.jsonl, data/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "PII_LABELS = [\n",
    "    \"CREDIT_CARD\",\n",
    "    \"PHONE\",\n",
    "    \"EMAIL\",\n",
    "    \"PERSON_NAME\",\n",
    "    \"DATE\",\n",
    "    \"CITY\",\n",
    "    \"LOCATION\",\n",
    "]\n",
    "\n",
    "FIRST_NAMES = [\"john\", \"emma\", \"arun\", \"sophia\", \"mike\", \"lisa\", \"priya\", \"rohan\"]\n",
    "LAST_NAMES = [\"doe\", \"kumar\", \"sharma\", \"patel\", \"lee\", \"gomez\", \"singh\"]\n",
    "CITIES = [\"chennai\", \"mumbai\", \"delhi\", \"bangalore\", \"london\", \"paris\"]\n",
    "LOCATIONS = [\"central park\", \"marine drive\", \"mg road\", \"downtown\", \"near railway station\"]\n",
    "\n",
    "MONTHS = [\n",
    "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
    "    \"july\", \"august\", \"september\", \"october\", \"november\", \"december\",\n",
    "]\n",
    "\n",
    "DIGITS = \"0123456789\"\n",
    "\n",
    "\n",
    "def random_credit_card():\n",
    "    # 16-digit card as a string (no spaces)\n",
    "    return \"\".join(random.choice(DIGITS) for _ in range(16))\n",
    "\n",
    "\n",
    "def random_phone():\n",
    "    # 10-digit phone\n",
    "    return \"\".join(random.choice(DIGITS) for _ in range(10))\n",
    "\n",
    "\n",
    "def random_email():\n",
    "    first = random.choice(FIRST_NAMES)\n",
    "    last = random.choice(LAST_NAMES)\n",
    "    domain = random.choice([\"gmail.com\", \"yahoo.com\", \"hotmail.com\", \"example.com\"])\n",
    "    # Simulate STT-style email\n",
    "    if random.random() < 0.5:\n",
    "        return f\"{first}.{last}@{domain}\"\n",
    "    else:\n",
    "        dom_user, dom_tld = domain.split(\".\")\n",
    "        return f\"{first} dot {last} at {dom_user} dot {dom_tld}\"\n",
    "\n",
    "\n",
    "def random_person():\n",
    "    return f\"{random.choice(FIRST_NAMES)} {random.choice(LAST_NAMES)}\"\n",
    "\n",
    "\n",
    "def random_date():\n",
    "    day = random.randint(1, 28)\n",
    "    month = random.choice(MONTHS)\n",
    "    year = random.randint(1995, 2025)\n",
    "    return f\"{day} {month} {year}\"\n",
    "\n",
    "\n",
    "def random_city():\n",
    "    return random.choice(CITIES)\n",
    "\n",
    "\n",
    "def random_location():\n",
    "    return random.choice(LOCATIONS)\n",
    "\n",
    "\n",
    "def generate_example(idx: int):\n",
    "    \"\"\"\n",
    "    Create one noisy STT-style utterance with 1–3 entities randomly inserted.\n",
    "    Returns dict: {id, text, entities}\n",
    "    \"\"\"\n",
    "    base_templates = [\n",
    "        \"please update my account details\",\n",
    "        \"can you check the status of my order\",\n",
    "        \"i want to change my registered phone number\",\n",
    "        \"this is regarding my previous payment\",\n",
    "        \"i need help with my booking\",\n",
    "        \"can you confirm my details again\",\n",
    "        \"i am talking from the customer care line\",\n",
    "        \"the delivery address has changed\",\n",
    "    ]\n",
    "    text_tokens = random.choice(base_templates).split()\n",
    "\n",
    "    num_entities = random.randint(1, 3)\n",
    "    entities = []\n",
    "\n",
    "    current_text = \"\"\n",
    "\n",
    "    # Decide insertion positions (word indices) for entities\n",
    "    insert_positions = sorted(\n",
    "        random.sample(range(len(text_tokens) + 1), num_entities)\n",
    "    )\n",
    "\n",
    "    pos_idx = 0\n",
    "    word_idx = 0\n",
    "\n",
    "    while word_idx <= len(text_tokens):\n",
    "        # Insert entity if needed at this index\n",
    "        if pos_idx < len(insert_positions) and insert_positions[pos_idx] == word_idx:\n",
    "            label = random.choice(PII_LABELS)\n",
    "            if label == \"CREDIT_CARD\":\n",
    "                ent_text = random_credit_card()\n",
    "            elif label == \"PHONE\":\n",
    "                ent_text = random_phone()\n",
    "            elif label == \"EMAIL\":\n",
    "                ent_text = random_email()\n",
    "            elif label == \"PERSON_NAME\":\n",
    "                ent_text = random_person()\n",
    "            elif label == \"DATE\":\n",
    "                ent_text = random_date()\n",
    "            elif label == \"CITY\":\n",
    "                ent_text = random_city()\n",
    "            else:\n",
    "                ent_text = random_location()\n",
    "\n",
    "            if current_text:\n",
    "                current_text += \" \"\n",
    "            start = len(current_text)\n",
    "            current_text += ent_text\n",
    "            end = len(current_text)\n",
    "\n",
    "            entities.append({\"start\": start, \"end\": end, \"label\": label})\n",
    "            pos_idx += 1\n",
    "\n",
    "        if word_idx < len(text_tokens):\n",
    "            # Add normal token\n",
    "            if current_text:\n",
    "                current_text += \" \"\n",
    "            token = text_tokens[word_idx]\n",
    "            start = len(current_text)\n",
    "            current_text += token\n",
    "            end = len(current_text)\n",
    "        word_idx += 1\n",
    "\n",
    "    return {\n",
    "        \"id\": f\"utt_{idx:05d}\",\n",
    "        \"text\": current_text,\n",
    "        \"entities\": entities,\n",
    "    }\n",
    "\n",
    "\n",
    "def write_jsonl(path, examples):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in examples:\n",
    "            f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    train_size = 800\n",
    "    dev_size = 150\n",
    "\n",
    "    train_examples = [generate_example(i) for i in range(train_size)]\n",
    "    dev_examples = [generate_example(i + train_size) for i in range(dev_size)]\n",
    "\n",
    "    write_jsonl(\"data/train.jsonl\", train_examples)\n",
    "    write_jsonl(\"data/dev.jsonl\", dev_examples)\n",
    "\n",
    "    # Dummy test file (you can change later)\n",
    "    write_jsonl(\"data/test.jsonl\", [{\"id\": \"test_00001\", \"text\": \"dummy text\", \"entities\": []}])\n",
    "\n",
    "    print(\"Generated: data/train.jsonl, data/dev.jsonl, data/test.jsonl\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808f6152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\joelm\\OneDrive\\Pictures\\Desktop\\PLIVO\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Change this path if your PLIVO folder is somewhere else\n",
    "project_dir = r\"C:\\Users\\joelm\\OneDrive\\Pictures\\Desktop\\PLIVO\"\n",
    "\n",
    "os.chdir(project_dir)\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b5b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 1)) (2.9.1+cpu)\n",
      "Requirement already satisfied: transformers in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 2)) (4.56.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 4)) (4.67.1)\n",
      "Collecting seqeval (from -r requirements.txt (line 5))\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers->-r requirements.txt (line 2)) (0.35.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joelm\\appdata\\roaming\\python\\python311\\site-packages (from transformers->-r requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers->-r requirements.txt (line 2)) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers->-r requirements.txt (line 2)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers->-r requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\joelm\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seqeval->-r requirements.txt (line 5)) (1.7.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 5)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers->-r requirements.txt (line 2)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers->-r requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joelm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers->-r requirements.txt (line 2)) (2025.8.3)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (pyproject.toml): started\n",
      "  Building wheel for seqeval (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16283 sha256=9fabbbf733b5d26e8ce661698684a31c198f06e2645a50704cb6de7e267af1e0\n",
      "  Stored in directory: c:\\users\\joelm\\appdata\\local\\pip\\cache\\wheels\\bc\\92\\f0\\243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\joelm\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -r requirements.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa952ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce94df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9a26198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 0.8247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 0.0423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 100/100 [00:55<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: 0.0117\n",
      "Saved model + tokenizer to out\n"
     ]
    }
   ],
   "source": [
    "%run train.py --model_name distilbert-base-uncased --train data/train.jsonl --dev data/dev.jsonl --out_dir out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd821f",
   "metadata": {},
   "source": [
    "## First experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f51de92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote predictions for 150 utterances to out/dev_pred.json\n"
     ]
    }
   ],
   "source": [
    "%run predict.py --model_dir out --input data/dev.jsonl --output out/dev_pred.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1de97f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-entity metrics:\n",
      "CITY            P=1.000 R=1.000 F1=1.000\n",
      "CREDIT_CARD     P=1.000 R=1.000 F1=1.000\n",
      "DATE            P=1.000 R=1.000 F1=1.000\n",
      "EMAIL           P=1.000 R=1.000 F1=1.000\n",
      "LOCATION        P=1.000 R=1.000 F1=1.000\n",
      "PERSON_NAME     P=1.000 R=1.000 F1=1.000\n",
      "PHONE           P=1.000 R=1.000 F1=1.000\n",
      "\n",
      "Macro-F1: 1.000\n",
      "\n",
      "PII-only metrics: P=1.000 R=1.000 F1=1.000\n",
      "Non-PII metrics: P=1.000 R=1.000 F1=1.000\n"
     ]
    }
   ],
   "source": [
    "%run eval_span_f1.py --gold data/dev.jsonl --pred out/dev_pred.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7bde18",
   "metadata": {},
   "source": [
    "## Second Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d62ac437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote predictions for 150 utterances to out/dev_pred.json\n"
     ]
    }
   ],
   "source": [
    "%run predict.py --model_dir out --input data/dev.jsonl --output out/dev_pred.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31404c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-entity metrics:\n",
      "CITY            P=1.000 R=1.000 F1=1.000\n",
      "CREDIT_CARD     P=1.000 R=1.000 F1=1.000\n",
      "DATE            P=1.000 R=1.000 F1=1.000\n",
      "EMAIL           P=1.000 R=1.000 F1=1.000\n",
      "LOCATION        P=1.000 R=1.000 F1=1.000\n",
      "PERSON_NAME     P=1.000 R=1.000 F1=1.000\n",
      "PHONE           P=1.000 R=1.000 F1=1.000\n",
      "\n",
      "Macro-F1: 1.000\n",
      "\n",
      "PII-only metrics: P=1.000 R=1.000 F1=1.000\n",
      "Non-PII metrics: P=1.000 R=1.000 F1=1.000\n"
     ]
    }
   ],
   "source": [
    "%run eval_span_f1.py --gold data/dev.jsonl --pred out/dev_pred.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "254c270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency over 50 runs (batch_size=1):\n",
      "  p50: 19.68 ms\n",
      "  p95: 22.28 ms\n"
     ]
    }
   ],
   "source": [
    "%run measure_latency.py --model_dir out --input data/dev.jsonl --runs 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5b7c8",
   "metadata": {},
   "source": [
    "## Second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9159faa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency over 50 runs (batch_size=1):\n",
      "  p50: 19.41 ms\n",
      "  p95: 22.71 ms\n"
     ]
    }
   ],
   "source": [
    "%run measure_latency.py --model_dir out --input data/dev.jsonl --runs 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40131674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1e2aa85",
   "metadata": {},
   "source": [
    "## ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54aea3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joelm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\joelm\\.cache\\huggingface\\hub\\models--albert-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 100/100 [00:55<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 0.5765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 100/100 [00:56<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 0.0290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 100/100 [00:55<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: 0.0045\n",
      "Saved model + tokenizer to out_albert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run train.py --model_name albert-base-v2 --train data/train.jsonl --dev data/dev.jsonl --out_dir out_albert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "673632d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote predictions for 150 utterances to out_albert/dev_pred.json\n",
      "Per-entity metrics:\n",
      "CITY            P=1.000 R=1.000 F1=1.000\n",
      "CREDIT_CARD     P=0.895 R=1.000 F1=0.944\n",
      "DATE            P=1.000 R=1.000 F1=1.000\n",
      "EMAIL           P=0.818 R=1.000 F1=0.900\n",
      "LOCATION        P=1.000 R=1.000 F1=1.000\n",
      "PERSON_NAME     P=0.929 R=1.000 F1=0.963\n",
      "PHONE           P=0.811 R=1.000 F1=0.896\n",
      "\n",
      "Macro-F1: 0.958\n",
      "\n",
      "PII-only metrics: P=0.882 R=1.000 F1=0.937\n",
      "Non-PII metrics: P=1.000 R=1.000 F1=1.000\n",
      "Latency over 50 runs (batch_size=1):\n",
      "  p50: 39.89 ms\n",
      "  p95: 61.35 ms\n"
     ]
    }
   ],
   "source": [
    "%run predict.py --model_dir out_albert --input data/dev.jsonl --output out_albert/dev_pred.json\n",
    "%run eval_span_f1.py --gold data/dev.jsonl --pred out_albert/dev_pred.json\n",
    "%run measure_latency.py --model_dir out_albert --input data/dev.jsonl --runs 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a4c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f1daf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 0.8134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 0.0446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 100/100 [00:56<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: 0.0142\n",
      "Saved model + tokenizer to out_distil128\n"
     ]
    }
   ],
   "source": [
    "%run train.py --model_name distilbert-base-uncased --train data/train.jsonl --dev data/dev.jsonl --out_dir out_distil128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3046c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote predictions for 150 utterances to out_distil128/dev_pred.json\n",
      "Per-entity metrics:\n",
      "CITY            P=1.000 R=1.000 F1=1.000\n",
      "CREDIT_CARD     P=1.000 R=1.000 F1=1.000\n",
      "DATE            P=1.000 R=1.000 F1=1.000\n",
      "EMAIL           P=1.000 R=1.000 F1=1.000\n",
      "LOCATION        P=1.000 R=1.000 F1=1.000\n",
      "PERSON_NAME     P=1.000 R=1.000 F1=1.000\n",
      "PHONE           P=1.000 R=1.000 F1=1.000\n",
      "\n",
      "Macro-F1: 1.000\n",
      "\n",
      "PII-only metrics: P=1.000 R=1.000 F1=1.000\n",
      "Non-PII metrics: P=1.000 R=1.000 F1=1.000\n",
      "Latency over 50 runs (batch_size=1):\n",
      "  p50: 21.48 ms\n",
      "  p95: 25.56 ms\n"
     ]
    }
   ],
   "source": [
    "%run predict.py --model_dir out_distil128 --input data/dev.jsonl --output out_distil128/dev_pred.json\n",
    "%run eval_span_f1.py --gold data/dev.jsonl --pred out_distil128/dev_pred.json\n",
    "%run measure_latency.py --model_dir out_distil128 --input data/dev.jsonl --runs 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f53eb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 100/100 [00:55<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 0.8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 100/100 [00:55<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 0.0411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: 0.0140\n",
      "Saved model + tokenizer to out_distil64\n"
     ]
    }
   ],
   "source": [
    "%run train.py --model_name distilbert-base-uncased --train data/train.jsonl --dev data/dev.jsonl --out_dir out_distil64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2f6fc04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote predictions for 150 utterances to out_distil64/dev_pred.json\n",
      "Per-entity metrics:\n",
      "CITY            P=1.000 R=1.000 F1=1.000\n",
      "CREDIT_CARD     P=1.000 R=1.000 F1=1.000\n",
      "DATE            P=1.000 R=1.000 F1=1.000\n",
      "EMAIL           P=1.000 R=1.000 F1=1.000\n",
      "LOCATION        P=1.000 R=1.000 F1=1.000\n",
      "PERSON_NAME     P=1.000 R=1.000 F1=1.000\n",
      "PHONE           P=1.000 R=1.000 F1=1.000\n",
      "\n",
      "Macro-F1: 1.000\n",
      "\n",
      "PII-only metrics: P=1.000 R=1.000 F1=1.000\n",
      "Non-PII metrics: P=1.000 R=1.000 F1=1.000\n",
      "Latency over 1 runs (batch_size=1):\n",
      "  p50: 17.04 ms\n",
      "  p95: 17.04 ms\n"
     ]
    }
   ],
   "source": [
    "%run predict.py --model_dir out_distil64 --input data/dev.jsonl --output out_distil64/dev_pred.json\n",
    "%run eval_span_f1.py --gold data/dev.jsonl --pred out_distil64/dev_pred.json\n",
    "%run measure_latency.py --model_dir out_distil64 --input data/dev.jsonl --runs 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a8317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ba45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
